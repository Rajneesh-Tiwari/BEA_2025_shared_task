{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import AdamW\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\"model_name\": \"microsoft/deberta-v3-large\",\n",
    "    \"max_len\": 2048,\n",
    "    \"freeze\" : False,\n",
    "    # Train Configs\n",
    "    \"fold_num\": 5,\n",
    "    \"val_fold\": 0,\n",
    "    \"learning_rate\": 2e-05,\n",
    "    \"min_lr\": 1e-7,\n",
    "    \"T_max\": 500,\n",
    "    \"valid_batch_size\": 16,\n",
    "    'train_batch_size' : 8,\n",
    " \n",
    "    \"epochs\": 25, # Set to 1 because it is a demo\n",
    "    \"accumulation_steps\": 1,\n",
    "    \"val_steps\": 375,\n",
    "    \"n_accumulate\":4,\n",
    "    \n",
    "    # GPU Optimize Settings\n",
    "    \"scheduler\" : 'cosine',\n",
    "    \"warmup_epochs\": 1,\n",
    "\n",
    "    \"gradient_checkpoint\" : False,\n",
    "    'tokenizer' : AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\"),\n",
    "    \n",
    "    # Path\n",
    "    \"output\": f\"///mnt/c/Personal/Competitions/BEA_2025/debetav3_context_multisampleDropout\",\n",
    "    \"seed\":42,\n",
    "}\n",
    "\n",
    "Path(cfg['output']).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(cfg['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_scheduler(model):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "                \"weight_decay\": 0.003,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        opt = AdamW(optimizer_parameters, lr=cfg['learning_rate'])\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for k in [\"Train Loss\", \"Valid Loss\"]:\n",
    "        plt.plot(history[k])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    \n",
    "    # Plot Metrics\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for k in [\"Macro F1\", \"Accuracy\"]:\n",
    "        plt.plot(history[k])\n",
    "    plt.title('Metrics')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('score')\n",
    "    plt.legend(['Macro F1', 'Accuracy'], loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385093cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tutor classes\n",
    "TUTOR_CLASSES = [\n",
    "    \"Expert\",\n",
    "    \"Novice\",\n",
    "    \"Gemini\",\n",
    "    \"GPT4\",\n",
    "    \"Llama31405B\",\n",
    "    \"Llama318B\",\n",
    "    \"Mistral\",\n",
    "    \"Phi3\",\n",
    "    \"Sonnet\"\n",
    "]\n",
    "\n",
    "# Create label mappings\n",
    "id2label = {i: label for i, label in enumerate(TUTOR_CLASSES)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cb6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dev_data_path='///mnt/c/Personal/Competitions/BEA_2025/data/mrbench_v3_devset.json'):\n",
    "    \"\"\"\n",
    "    Load development and (optionally) test datasets\n",
    "    \"\"\"\n",
    "    # Load development data\n",
    "    with open(dev_data_path, 'r') as f:\n",
    "        dev_data = json.load(f)\n",
    "    \n",
    "    # Process development data\n",
    "    dev_examples = []\n",
    "    for dialogue in dev_data:\n",
    "        conversation_id = dialogue[\"conversation_id\"]\n",
    "        conversation_history = dialogue[\"conversation_history\"]\n",
    "        \n",
    "        for tutor_id, tutor_data in dialogue[\"tutor_responses\"].items():\n",
    "            if tutor_id in TUTOR_CLASSES or any(cls_name in tutor_id for cls_name in TUTOR_CLASSES):\n",
    "                # Map the tutor_id to one of our classes\n",
    "                tutor_class = next((cls for cls in TUTOR_CLASSES if cls in tutor_id), tutor_id)\n",
    "                \n",
    "                dev_examples.append({\n",
    "                    \"conversation_id\": conversation_id,\n",
    "                    \"conversation_history\": conversation_history,\n",
    "                    \"tutor_response\": tutor_data[\"response\"],\n",
    "                    \"tutor_class\": tutor_class\n",
    "                })\n",
    "    return dev_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(load_data())\n",
    "train['target'] = train['tutor_class'].map(label2id)\n",
    "train.rename(columns={'conversation_history':'Question','tutor_response':'Response'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05844e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in train.iterrows():\n",
    "    question = row.Question\n",
    "    response = row.Response\n",
    "    convid = row.conversation_id\n",
    "    context = train[(train.conversation_id==convid)&(train.Response!=response)].Response.values\n",
    "    context = ' [SEP] '.join(context)\n",
    "    train.loc[i, 'context'] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Question'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset) :\n",
    "    def __init__(self,df,is_train = True, use_aug=False) :\n",
    "        self.use_aug = use_aug\n",
    "        self.df = df\n",
    "        self.tokenizer = cfg['tokenizer']\n",
    "        self.sep_token = self.tokenizer.sep_token\n",
    "        self.text = (self.df['Question'] + self.sep_token + '[R_STRAT]' + self.df['Response']+ '[R_END]'+ self.sep_token + self.df['context']).values\n",
    "\n",
    "        if is_train==True :\n",
    "            self.targets = df['target'].values\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.text[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=cfg[\"max_len\"]\n",
    "        )\n",
    "        feature_dict = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "        if self.is_train==True :\n",
    "            feature_dict['target'] =  self.targets[index]\n",
    "            \n",
    "        custom_mask = inputs['attention_mask'].copy()\n",
    "        stop_idx = 0\n",
    "        for idx, token in enumerate(inputs['input_ids']):\n",
    "                if token == cfg[\"tokenizer\"].convert_tokens_to_ids('[R_END]'):\n",
    "                    stop_idx = idx + 1\n",
    "                    break\n",
    "        for idx in range(stop_idx, len(inputs['attention_mask'])):\n",
    "            custom_mask[idx] = 0\n",
    "        feature_dict['R_mask'] = custom_mask\n",
    "\n",
    "        return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"tokenizer\"].add_tokens(['[R_STRAT]', '[R_END]'], special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_layer_freeze(module):\n",
    "    for i in range(1,24,2):\n",
    "        for n,p in module.encoder.layer[i].named_parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "def even_layer_freeze(module):\n",
    "    for i in range(0,24,2):\n",
    "        for n,p in module.encoder.layer[i].named_parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "def top_half_layer_freeze(module):\n",
    "    for i in range(0,13,1):\n",
    "        for n,p in module.encoder.layer[i].named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "def bottom_half_layer_freeze(module):\n",
    "    for i in range(13,14,1):\n",
    "        for n,p in module.encoder.layer[i].named_parameters():\n",
    "            p.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim=1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n",
    "        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret\n",
    "    \n",
    "class MultiSampleDropout(nn.Module):\n",
    "    def __init__(self, classifier, start_prob=0.2, num_samples=8, increment=0.01):\n",
    "        super(MultiSampleDropout, self).__init__()\n",
    "        # Use standard nn.Dropout since we're integrating with the first notebook\n",
    "        self.dropouts = nn.ModuleList([\n",
    "            nn.Dropout(start_prob + (increment*i)) for i in range(num_samples)\n",
    "        ])\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, out):\n",
    "        # Apply multiple dropouts and average results\n",
    "        return torch.mean(torch.stack([\n",
    "            self.classifier(dropout(out)) for dropout in self.dropouts\n",
    "        ], dim=0), dim=0)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.drop = nn.Dropout(p=cfg[\"dropout\"])\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(cfg[\"model_name\"])\n",
    "        #self.config.hidden_dropout = 0.\n",
    "        self.config.hidden_dropout_prob = 0.007\n",
    "        #self.config.attention_dropout = 0.\n",
    "        self.config.attention_probs_dropout_prob = 0.008\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(cfg[\"model_name\"], config=self.config)\n",
    "        #odd_layer_freeze(self.model)\n",
    "        if cfg[\"gradient_checkpoint\"]:\n",
    "            print('Enabling Grad Checkpointing')\n",
    "            self.model.gradient_checkpointing_enable()  \n",
    "        if cfg[\"freeze\"]:\n",
    "            print('freezing params')\n",
    "            for parameter in self.model.parameters():\n",
    "                parameter.requires_grad = False\n",
    "        self.pool = GeMText()\n",
    "\n",
    "        # Create a classifier (single linear layer)\n",
    "        self.fc_base = nn.Linear(self.config.hidden_size, len(id2label))\n",
    "\n",
    "        # Wrap it with Multi-Sample Dropout\n",
    "        self.fc = MultiSampleDropout(self.fc_base, \n",
    "                                    start_prob=0.2, \n",
    "                                    num_samples=8, \n",
    "                                    increment=0.01)\n",
    "        \n",
    "    def forward(self, ids, mask, rhead):        \n",
    "        out = self.model(input_ids=ids,attention_mask=mask,\n",
    "                         output_hidden_states=False)\n",
    "        out = self.pool(out.last_hidden_state, rhead)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        output[\"R_mask\"] = [sample[\"R_mask\"] for sample in batch]\n",
    "        output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]]) \n",
    "\n",
    "        batch_max = min(batch_max, self.max_len)\n",
    "        \n",
    "        output[\"input_ids\"] = [s[:batch_max] for s in output[\"input_ids\"]]\n",
    "        output[\"attention_mask\"] = [s[:batch_max] for s in output[\"attention_mask\"]] \n",
    "        output[\"R_mask\"] = [s[:batch_max] for s in output[\"R_mask\"]] \n",
    "        #output[\"target\"] = [s[:batch_max] for s in output[\"target\"]]\n",
    "\n",
    "\n",
    "        output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "        output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        output[\"R_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"R_mask\"]]\n",
    "        #output[\"target\"] = [s + (batch_max - len(s)) * [0] for s in output[\"target\"]]\n",
    "        \n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "        output[\"R_mask\"] = torch.tensor(output[\"R_mask\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device):\n",
    "    model.train()\n",
    "\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    losses = AverageMeter()\n",
    "    scaler = torch.amp.GradScaler(enabled=cfg[\"apex\"])\n",
    "    lr = []\n",
    "    bar = tqdm(dataloader, total=len(dataloader))\n",
    "    steps = len(dataloader)\n",
    "    \n",
    "    all_preds = np.array([])\n",
    "    all_groud_truth = np.array([])\n",
    "    \n",
    "    for step, data in enumerate(bar):\n",
    "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"target\"].to(device, dtype=torch.long)\n",
    "        r_mask = data[\"R_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        batch_size = ids.size(0)\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "        # with torch.amp.autocast(enabled=cfg[\"apex\"],device_type=\"cuda\"):\n",
    "            outputs = model(ids, mask, r_mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "        loss = loss / cfg['n_accumulate']\n",
    "        \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % cfg['n_accumulate'] == 0 or step == steps:\n",
    "            scaler.unscale_(optimizer)\n",
    "            # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_norm\"])\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_norm\"])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "        epoch_loss = losses.avg\n",
    "        #acc = correct / total\n",
    "\n",
    "        bar.set_postfix(\n",
    "            Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        lr.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    y_test = []\n",
    "    for data in dataloader:\n",
    "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"target\"].to(device, dtype=torch.long)\n",
    "        r_mask = data[\"R_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        batch_size = ids.size(0)\n",
    "\n",
    "        outputs = model(ids, mask, r_mask)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(outputs.detach().cpu().numpy())\n",
    "        y_test.append(targets.detach().cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds)\n",
    "    y_test = np.concatenate(y_test)\n",
    "    \n",
    "    # Get the predicted class (argmax)\n",
    "    pred_labels = np.argmax(preds, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, pred_labels)\n",
    "    \n",
    "    # Calculate macro F1 score\n",
    "    macro_f1 = f1_score(y_test, pred_labels, average='macro')\n",
    "\n",
    "    return losses.avg, preds, y_test, macro_f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(model, optimizer, scheduler, device, num_epochs, train_loader, valid_loader, fold=0):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    best_score = 0  # Changed to 0 since higher F1 is better\n",
    "    history = {\"Train Loss\": [], \"Valid Loss\": [], \"Macro F1\": [], \"Accuracy\": []}\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        train_epoch_loss = train_one_epoch(\n",
    "            model, optimizer, scheduler, dataloader=train_loader, device=cfg[\"device\"]\n",
    "        )\n",
    "\n",
    "        val_epoch_loss, preds, y_test, macro_f1, accuracy = evaluate(\n",
    "            model, valid_loader, device=cfg[\"device\"]\n",
    "        )\n",
    "        \n",
    "        # print(f\"Epoch {epoch}: Loss={val_epoch_loss:.4f}, Macro F1={macro_f1:.4f}, Accuracy={accuracy:.4f}\")\n",
    "        print(f\"Epoch {epoch}: Train Loss={train_epoch_loss:.4f}, Valid Loss={val_epoch_loss:.4f}, Macro F1={macro_f1:.4f}, Accuracy={accuracy:.4f}\")\n",
    "        \n",
    "        # Use Macro F1 as the primary score for model selection\n",
    "        score = macro_f1\n",
    "\n",
    "        history[\"Train Loss\"].append(train_epoch_loss)\n",
    "        history[\"Valid Loss\"].append(val_epoch_loss)\n",
    "        history[\"Macro F1\"].append(macro_f1)\n",
    "        history[\"Accuracy\"].append(accuracy)\n",
    "\n",
    "        # For Macro F1, higher is better, so we change the comparison\n",
    "        if score >= best_score:\n",
    "            print(\n",
    "                f\"Score Improved ({best_score:.4f} ---> {score:.4f})\"\n",
    "            )\n",
    "            with open(f\"{cfg['output']}/log.txt\", 'a') as f:\n",
    "                # f.write(f'Epoch {epoch}: Loss={val_epoch_loss:.4f}, Macro F1={macro_f1:.4f}, Accuracy={accuracy:.4f}\\n')\n",
    "                f.write(f'Epoch {epoch}: Train Loss={train_epoch_loss:.4f}, Valid Loss={val_epoch_loss:.4f}, Macro F1={macro_f1:.4f}, Accuracy={accuracy:.4f}\\n')\n",
    "\n",
    "            best_score = score\n",
    "            PATH = os.path.join(cfg['output'],f'fold_{fold}.bin')\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            \n",
    "            print(f\"Model Saved\")\n",
    "            best_y = preds\n",
    "\n",
    "        print()\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print(\n",
    "        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 3600,\n",
    "            (time_elapsed % 3600) // 60,\n",
    "            (time_elapsed % 3600) % 60,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Best Macro F1 Score: {:.4f}\".format(\n",
    "            best_score\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return history, best_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef87050",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "folds = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train['fold'] = -1\n",
    "for i,(train_index, test_index) in enumerate(folds.split(train,train['target'], groups=train['conversation_id'])): \n",
    "    train.loc[test_index,'fold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg['n_accumulate'] = 4#5\n",
    "cfg['dropout'] = 0.4\n",
    "cfg['apex'] = True\n",
    "cfg[\"grad_norm\"] = 20\n",
    "cfg[\"gradient_checkpoint\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9562cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import Parameter\n",
    "import time\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_folds() :\n",
    "\n",
    "    for fold in range(5) :\n",
    "\n",
    "        print(f'--------------------------------Training Fold {fold+1}/5---------------------------------')\n",
    "        with open(f\"{cfg['output']}/log.txt\", 'a') as f:\n",
    "                f.write(f'fold {fold+1}/5 \\n')\n",
    "        train_ = train[train.fold!=fold].reset_index(drop=True)\n",
    "        valid_ = train[train.fold==fold].reset_index(drop=True)\n",
    "        \n",
    "        print(f'train shape : {len(train_)}')\n",
    "        print(f'valid shape : {len(valid_)}')\n",
    "        \n",
    "        train_dataset = Dataset(\n",
    "                                    train_, True, True\n",
    "                            )\n",
    "        valid_dataset = Dataset(\n",
    "                                    valid_,True\n",
    "        )\n",
    "        collate_fn = Collate(tokenizer=cfg['tokenizer'])\n",
    "        #Collate(tokenizer=cfg['tokenizer'])#DataCollatorWithPadding(tokenizer=cfg['tokenizer'])\n",
    "        train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=cfg[\"train_batch_size\"],\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=8,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "                drop_last=True\n",
    "                    )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=cfg[\"valid_batch_size\"],\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=8,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        \n",
    "        model = Model()\n",
    "        model.to(cfg['device'])\n",
    "\n",
    "        steps = len(train_loader)\n",
    "        total_steps = steps * cfg['epochs']\n",
    "        optimizer = optimizer_scheduler(model)\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "        #                 optimizer, T_max=cfg['T_max'], eta_min=cfg['min_lr'])\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=cfg['learning_rate'], \n",
    "                                            total_steps=total_steps//cfg['n_accumulate'])\n",
    "        history = start_training(\n",
    "                        model, optimizer, scheduler, cfg['device'], cfg['epochs'] ,train_loader=train_loader,valid_loader=valid_loader,fold=fold)\n",
    "        torch.cuda.empty_cache()\n",
    "        del model, optimizer, scheduler, train_loader, valid_loader, train_dataset, valid_dataset, collate_fn\n",
    "        #plot_history(history)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [f'target_{i}' for i in range(len(id2label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    losses = AverageMeter()\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    y_test = []\n",
    "    bar = tqdm(dataloader, total=len(dataloader))\n",
    "    for data in bar:\n",
    "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        rmask = data[\"R_mask\"].to(device, dtype=torch.long)\n",
    "        batch_size = ids.size(0)\n",
    "\n",
    "        outputs = model(ids, mask, rmask)\n",
    "        preds.append(outputs.softmax(dim=1).detach().cpu().numpy())\n",
    "        \n",
    "    \n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof() :\n",
    "    oof = train.copy(deep=True)\n",
    "    for fold in range(5) :\n",
    "\n",
    "        print(f'--------------------------------Inferring Fold {fold+1}/5---------------------------------')\n",
    "        valid_ = train[train.fold==fold]\n",
    "        idxs = valid_.index\n",
    "        valid_ = valid_.reset_index()\n",
    "        \n",
    "        print(f'valid shape : {len(valid_)}')\n",
    "        \n",
    "        valid_dataset = Dataset(valid_,True)\n",
    "        collate_fn = Collate(tokenizer=cfg['tokenizer'])#DataCollatorWithPadding(tokenizer=cfg['tokenizer'])\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=cfg[\"valid_batch_size\"],\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=8,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        \n",
    "        model = Model()\n",
    "        model.to(cfg['device'])\n",
    "        model.load_state_dict(\n",
    "            torch.load(f\"///mnt/c/Personal/Competitions/BEA_2025/debetav3_context_multisampleDropout/fold_{fold}.bin\")\n",
    "        )\n",
    "        y_test = infer(\n",
    "            model, valid_loader, device=cfg[\"device\"]\n",
    "        )\n",
    "        oof.loc[idxs, target_cols] = y_test\n",
    "        torch.cuda.empty_cache()\n",
    "    return oof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof = get_oof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fce9603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64def29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5afc8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# oof['pred'] = oof[target_cols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[1])\n",
    "# oof['pred'] = oof['pred'].astype(int)\n",
    "\n",
    "# print(f1_score(oof['target'], oof['pred'], average='macro'))\n",
    "# print(accuracy_score(oof['target'], oof['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof.to_csv('///mnt/c/Personal/Competitions/BEA_2025/debetav3_context_multisampleDropout/oofs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06c1d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Processing Functions\n",
    "def load_test_data(test_data_path='///mnt/c/Personal/Competitions/BEA_2025/data/mrbench_v3_testset.json'):\n",
    "    \"\"\"\n",
    "    Load development and (optionally) test datasets\n",
    "    \"\"\"    \n",
    "    # Load test data if provided\n",
    "    test_examples = []\n",
    "    test_data = None\n",
    "    if test_data_path:\n",
    "        with open(test_data_path, 'r',encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "        \n",
    "        for dialogue in test_data:\n",
    "            conversation_id = dialogue[\"conversation_id\"]\n",
    "            conversation_history = dialogue[\"conversation_history\"]\n",
    "            \n",
    "            for tutor_id, tutor_data in dialogue[\"tutor_responses\"].items():\n",
    "                test_examples.append({\n",
    "                    \"conversation_id\": conversation_id,\n",
    "                    \"conversation_history\": conversation_history,\n",
    "                    \"tutor_response\": tutor_data[\"response\"],\n",
    "                    \"tutor_id\": tutor_id\n",
    "                })\n",
    "    \n",
    "    return test_examples, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples,test_data = load_test_data()\n",
    "# test\n",
    "# test['Response'] = test['Response'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test_examples)\n",
    "test.rename(columns={'conversation_history':'Question','tutor_response':'Response'},inplace=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b993e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in test.iterrows():\n",
    "    question = row.Question\n",
    "    response = row.Response\n",
    "    convid = row.conversation_id\n",
    "    context = test[(test.conversation_id==convid)&(test.Response!=response)].Response.values\n",
    "    context = ' [SEP] '.join(context)\n",
    "    test.loc[i, 'context'] = context\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer, max_len=cfg['max_len']):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        output[\"R_mask\"] = [sample[\"R_mask\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]]) \n",
    "\n",
    "        batch_max = min(batch_max, self.max_len)\n",
    "        \n",
    "        output[\"input_ids\"] = [s[:batch_max] for s in output[\"input_ids\"]]\n",
    "        output[\"attention_mask\"] = [s[:batch_max] for s in output[\"attention_mask\"]] \n",
    "        output[\"R_mask\"] = [s[:batch_max] for s in output[\"R_mask\"]] \n",
    "        #output[\"target\"] = [s[:batch_max] for s in output[\"target\"]]\n",
    "\n",
    "\n",
    "        output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "        output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        output[\"R_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"R_mask\"]]\n",
    "        #output[\"target\"] = [s + (batch_max - len(s)) * [0] for s in output[\"target\"]]\n",
    "        \n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        output[\"R_mask\"] = torch.tensor(output[\"R_mask\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = Dataset(test,False )\n",
    "collate_fn = Collate(tokenizer=cfg['tokenizer'])\n",
    "test_loader = DataLoader(\n",
    "                valid_dataset,\n",
    "                batch_size=cfg[\"valid_batch_size\"],\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=8,\n",
    "                shuffle=False,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "final_preds = []\n",
    "for fold in range(5):\n",
    "    model = Model()\n",
    "    model.to(cfg['device'])\n",
    "    model.load_state_dict(\n",
    "        torch.load(f'///mnt/c/Personal/Competitions/BEA_2025/debetav3_context_multisampleDropout/fold_{fold}.bin')\n",
    "    )\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    preds = infer(model, test_loader, 'cuda')\n",
    "    final_preds.append(preds)\n",
    "\n",
    "final_preds = np.mean(final_preds, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [f'target_{i}' for i in range(len(id2label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[target_cols] = final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac36157",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred'] = test[target_cols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[1])\n",
    "test['pred'] = test['pred'].astype(int)\n",
    "test['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa5c4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('///mnt/c/Personal/Competitions/BEA_2025/debetav3_context_multisampleDropout/test_probas.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a2252b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = test['pred'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37390fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = []\n",
    "unique_conversation_ids = list(ex[\"conversation_id\"] for ex in test_examples)\n",
    "\n",
    "for conversation_id in unique_conversation_ids:\n",
    "    conversation_data = next(d for d in test_data if d[\"conversation_id\"] == conversation_id)\n",
    "    submission_item = {\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"conversation_history\": conversation_data[\"conversation_history\"],\n",
    "        \"tutor_responses\": {}\n",
    "    }\n",
    "        \n",
    "    for tutor_id, tutor_data in conversation_data[\"tutor_responses\"].items():\n",
    "        # Find the corresponding prediction\n",
    "        idx = next(i for i, ex in enumerate(test_examples) \n",
    "                    if ex[\"conversation_id\"] == conversation_id and ex[\"tutor_id\"] == tutor_id)\n",
    "        \n",
    "        predicted_class = id2label[pred_labels[idx]]\n",
    "        \n",
    "        submission_item[\"tutor_responses\"][tutor_id] = {\n",
    "            \"response\": tutor_data[\"response\"],\n",
    "            \"annotation\": {\n",
    "                \"Tutor_Identification\": predicted_class\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    submission.append(submission_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"///mnt/c/Personal/Competitions/BEA_2025/debetav3_context_multisampleDropout\", \"predictions.json\"), \"w\") as f:\n",
    "    json.dump(submission, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof['conversation_id'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87065e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers accelerate peft bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b40e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.kaggle.com/code/johnsonhk88/multilingual-chatbot-arena-llm-fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7604ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 22 13:00:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "|  0%   44C    P8             11W /  450W |     725MiB /  24564MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        27      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b853bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_MODEL = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45cf8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajneesh/miniconda3/envs/hf_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType # type: ignore\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy  as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['hf_cache '] = '///mnt/c/Personal/Competitions/HFCache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44141716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2cf5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Expert', 1: 'Novice', 2: 'Gemini', 3: 'GPT4', 4: 'Llama31405B', 5: 'Llama318B', 6: 'Mistral', 7: 'Phi3', 8: 'Sonnet'} {'Expert': 0, 'Novice': 1, 'Gemini': 2, 'GPT4': 3, 'Llama31405B': 4, 'Llama318B': 5, 'Mistral': 6, 'Phi3': 7, 'Sonnet': 8}\n"
     ]
    }
   ],
   "source": [
    "# Define the tutor classes\n",
    "TUTOR_CLASSES = [\n",
    "    \"Expert\",\n",
    "    \"Novice\",\n",
    "    \"Gemini\",\n",
    "    \"GPT4\",\n",
    "    \"Llama31405B\",\n",
    "    \"Llama318B\",\n",
    "    \"Mistral\",\n",
    "    \"Phi3\",\n",
    "    \"Sonnet\"\n",
    "]\n",
    "\n",
    "# Create label mappings\n",
    "id2label = {i: label for i, label in enumerate(TUTOR_CLASSES)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b5af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Processing Functions\n",
    "def load_data(dev_data_path='///mnt/c/Personal/Competitions/BEA_2025/data/mrbench_v3_devset.json'):\n",
    "    \"\"\"\n",
    "    Load development and (optionally) test datasets\n",
    "    \"\"\"\n",
    "    # Load development data\n",
    "    with open(dev_data_path, 'r') as f:\n",
    "        dev_data = json.load(f)\n",
    "    \n",
    "    # Process development data\n",
    "    dev_examples = []\n",
    "    for dialogue in dev_data:\n",
    "        conversation_id = dialogue[\"conversation_id\"]\n",
    "        conversation_history = dialogue[\"conversation_history\"]\n",
    "        \n",
    "        for tutor_id, tutor_data in dialogue[\"tutor_responses\"].items():\n",
    "            if tutor_id in TUTOR_CLASSES or any(cls_name in tutor_id for cls_name in TUTOR_CLASSES):\n",
    "                # Map the tutor_id to one of our classes\n",
    "                tutor_class = next((cls for cls in TUTOR_CLASSES if cls in tutor_id), tutor_id)\n",
    "                \n",
    "                dev_examples.append({\n",
    "                    \"conversation_id\": conversation_id,\n",
    "                    \"conversation_history\": conversation_history,\n",
    "                    \"tutor_response\": tutor_data[\"response\"],\n",
    "                    \"tutor_class\": tutor_class\n",
    "                })\n",
    "    \n",
    "\n",
    "    return dev_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13df65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.DataFrame(load_data())\n",
    "train['target'] = train['tutor_class'].map(label2id)\n",
    "train.rename(columns={'conversation_history':'Question','tutor_response':'Response'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d09386ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>Question</th>\n",
       "      <th>Response</th>\n",
       "      <th>tutor_class</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>Sonnet</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Now that we know the cost of 1 pound of meat i...</td>\n",
       "      <td>Llama318B</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>You're close, but I notice that you calculated...</td>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>That's correct. So, if 1 pound of meat costs $...</td>\n",
       "      <td>GPT4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>It seems like you've calculated the cost as if...</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            conversation_id  \\\n",
       "0  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "1  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "2  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "3  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "4  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "\n",
       "                                            Question  \\\n",
       "0  Tutor: Hi, could you please provide a step-by-...   \n",
       "1  Tutor: Hi, could you please provide a step-by-...   \n",
       "2  Tutor: Hi, could you please provide a step-by-...   \n",
       "3  Tutor: Hi, could you please provide a step-by-...   \n",
       "4  Tutor: Hi, could you please provide a step-by-...   \n",
       "\n",
       "                                            Response  tutor_class  target  \n",
       "0  Great, you've correctly identified the cost of...       Sonnet       8  \n",
       "1  Now that we know the cost of 1 pound of meat i...    Llama318B       5  \n",
       "2  You're close, but I notice that you calculated...  Llama31405B       4  \n",
       "3  That's correct. So, if 1 pound of meat costs $...         GPT4       3  \n",
       "4  It seems like you've calculated the cost as if...      Mistral       6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e474319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in train.iterrows():\n",
    "    question = row.Question\n",
    "    response = row.Response\n",
    "    convid = row.conversation_id\n",
    "    context = train[(train.conversation_id==convid)&(train.Response!=response)].Response.values\n",
    "    context = ' [SEP] '.join(context)\n",
    "    train.loc[i, 'context'] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b16da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_map = pd.read_csv('///mnt/c/Personal/Competitions/BEA_2025/debetav3_context_multisampleDropout/oofs.csv')\n",
    "\n",
    "train = pd.merge(train,fold_map[['conversation_id','fold','tutor_class']],on=['conversation_id','tutor_class'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb019c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['input'] = \"Question: \" + train['Question'] + '; Answer: ' + train['Response'] + '; Context: ' + train[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e3af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'target': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb5149f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>Question</th>\n",
       "      <th>Response</th>\n",
       "      <th>tutor_class</th>\n",
       "      <th>label</th>\n",
       "      <th>context</th>\n",
       "      <th>fold</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>Sonnet</td>\n",
       "      <td>8</td>\n",
       "      <td>Now that we know the cost of 1 pound of meat i...</td>\n",
       "      <td>0</td>\n",
       "      <td>Question: Tutor: Hi, could you please provide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Now that we know the cost of 1 pound of meat i...</td>\n",
       "      <td>Llama318B</td>\n",
       "      <td>5</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>0</td>\n",
       "      <td>Question: Tutor: Hi, could you please provide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>You're close, but I notice that you calculated...</td>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>4</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>0</td>\n",
       "      <td>Question: Tutor: Hi, could you please provide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>That's correct. So, if 1 pound of meat costs $...</td>\n",
       "      <td>GPT4</td>\n",
       "      <td>3</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>0</td>\n",
       "      <td>Question: Tutor: Hi, could you please provide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>It seems like you've calculated the cost as if...</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>6</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>0</td>\n",
       "      <td>Question: Tutor: Hi, could you please provide ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            conversation_id  \\\n",
       "0  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "1  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "2  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "3  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "4  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "\n",
       "                                            Question  \\\n",
       "0  Tutor: Hi, could you please provide a step-by-...   \n",
       "1  Tutor: Hi, could you please provide a step-by-...   \n",
       "2  Tutor: Hi, could you please provide a step-by-...   \n",
       "3  Tutor: Hi, could you please provide a step-by-...   \n",
       "4  Tutor: Hi, could you please provide a step-by-...   \n",
       "\n",
       "                                            Response  tutor_class  label  \\\n",
       "0  Great, you've correctly identified the cost of...       Sonnet      8   \n",
       "1  Now that we know the cost of 1 pound of meat i...    Llama318B      5   \n",
       "2  You're close, but I notice that you calculated...  Llama31405B      4   \n",
       "3  That's correct. So, if 1 pound of meat costs $...         GPT4      3   \n",
       "4  It seems like you've calculated the cost as if...      Mistral      6   \n",
       "\n",
       "                                             context  fold  \\\n",
       "0  Now that we know the cost of 1 pound of meat i...     0   \n",
       "1  Great, you've correctly identified the cost of...     0   \n",
       "2  Great, you've correctly identified the cost of...     0   \n",
       "3  Great, you've correctly identified the cost of...     0   \n",
       "4  Great, you've correctly identified the cost of...     0   \n",
       "\n",
       "                                               input  \n",
       "0  Question: Tutor: Hi, could you please provide ...  \n",
       "1  Question: Tutor: Hi, could you please provide ...  \n",
       "2  Question: Tutor: Hi, could you please provide ...  \n",
       "3  Question: Tutor: Hi, could you please provide ...  \n",
       "4  Question: Tutor: Hi, could you please provide ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cae23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with 4bit bnb\n",
    "\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType # type: ignore\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e91c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, Cache, Qwen2PreTrainedModel, Qwen2Model\n",
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Union, List, Tuple\n",
    "from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss\n",
    "\n",
    "class Qwen2ForSequenceClassificationPlus(Qwen2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = Qwen2Model(config)\n",
    "\n",
    "        self.score = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(config.hidden_size, config.hidden_size // 2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size // 2, self.num_labels),\n",
    "        )\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        logits = self.score(hidden_states)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "\n",
    "        if self.config.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n",
    "                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n",
    "                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n",
    "                sequence_lengths = sequence_lengths.to(logits.device)\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "\n",
    "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(pooled_logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(pooled_logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a499d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
    "tokenizer.add_eos_token = True\n",
    "# tokenizer.truncation_side = \"right\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# processor.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46baf194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        \n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            print(_)\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "561824b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, max_length=4096):\n",
    "    return tokenizer(examples[\"input\"], truncation=True, max_length=max_length, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33f77666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e12a26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    \n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    \n",
    "    # Add per-class F1 scores\n",
    "    f1_per_class = f1.compute(predictions=predictions, references=labels, average=None)\n",
    "    per_class_scores = {f\"f1_{id2label[i]}\": score for i, score in enumerate(f1_per_class[\"f1\"])}\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score[\"accuracy\"],\n",
    "        \"f1_macro\": f1_score[\"f1\"],\n",
    "        **per_class_scores\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a3f5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07a3e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "507916b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "def find_all_linear_modules(model) -> tp.List[str]:\n",
    "    r\"\"\"\n",
    "    Finds all available modules to apply lora.\n",
    "    \"\"\"\n",
    "    linear_cls = torch.nn.Linear\n",
    "\n",
    "    output_layer_names = [\"lm_head\", \"embed_tokens\"]\n",
    "\n",
    "    module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, linear_cls) and not any(\n",
    "            [output_layer in name for output_layer in output_layer_names]\n",
    "        ):\n",
    "            module_names.add(name.split(\".\")[-1])\n",
    "    return list(module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d04333e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1404146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1982/1982 [00:01<00:00, 1935.83 examples/s]\n",
      "Map: 100%|██████████| 494/494 [00:00<00:00, 2618.25 examples/s]\n",
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/362353729.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 32,846,217 || all params: 527,284,882 || trainable%: 6.2293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 33:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Expert</th>\n",
       "      <th>F1 Novice</th>\n",
       "      <th>F1 Gemini</th>\n",
       "      <th>F1 Gpt4</th>\n",
       "      <th>F1 Llama31405b</th>\n",
       "      <th>F1 Llama318b</th>\n",
       "      <th>F1 Mistral</th>\n",
       "      <th>F1 Phi3</th>\n",
       "      <th>F1 Sonnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.979300</td>\n",
       "      <td>1.123276</td>\n",
       "      <td>0.617409</td>\n",
       "      <td>0.627211</td>\n",
       "      <td>0.776978</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.451064</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.582524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.762200</td>\n",
       "      <td>0.633783</td>\n",
       "      <td>0.757085</td>\n",
       "      <td>0.763488</td>\n",
       "      <td>0.873950</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.809160</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.604027</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.630435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.536888</td>\n",
       "      <td>0.834008</td>\n",
       "      <td>0.838631</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>0.578309</td>\n",
       "      <td>0.844130</td>\n",
       "      <td>0.849446</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.910569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.201900</td>\n",
       "      <td>0.580704</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>0.860708</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.759124</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.884956</td>\n",
       "      <td>0.907563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.778616</td>\n",
       "      <td>0.864372</td>\n",
       "      <td>0.867653</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.824427</td>\n",
       "      <td>0.699029</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.924370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.821473</td>\n",
       "      <td>0.872470</td>\n",
       "      <td>0.879742</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.728972</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.902655</td>\n",
       "      <td>0.936508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.806361</td>\n",
       "      <td>0.872470</td>\n",
       "      <td>0.877758</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.813999</td>\n",
       "      <td>0.870445</td>\n",
       "      <td>0.878193</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.907563</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.873950</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.943089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.822098</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.876379</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.950820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.826610</td>\n",
       "      <td>0.870445</td>\n",
       "      <td>0.878311</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.950820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.824856</td>\n",
       "      <td>0.870445</td>\n",
       "      <td>0.878311</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.950820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1982/1982 [00:00<00:00, 2511.54 examples/s]\n",
      "Map: 100%|██████████| 494/494 [00:00<00:00, 3223.81 examples/s]\n",
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/362353729.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 32,846,217 || all params: 527,284,882 || trainable%: 6.2293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 33:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Expert</th>\n",
       "      <th>F1 Novice</th>\n",
       "      <th>F1 Gemini</th>\n",
       "      <th>F1 Gpt4</th>\n",
       "      <th>F1 Llama31405b</th>\n",
       "      <th>F1 Llama318b</th>\n",
       "      <th>F1 Mistral</th>\n",
       "      <th>F1 Phi3</th>\n",
       "      <th>F1 Sonnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.177000</td>\n",
       "      <td>1.290359</td>\n",
       "      <td>0.550607</td>\n",
       "      <td>0.569037</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.396694</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.773723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.668793</td>\n",
       "      <td>0.777328</td>\n",
       "      <td>0.781503</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.494845</td>\n",
       "      <td>0.776978</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.877193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.552300</td>\n",
       "      <td>0.421013</td>\n",
       "      <td>0.844130</td>\n",
       "      <td>0.844547</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.582588</td>\n",
       "      <td>0.831984</td>\n",
       "      <td>0.834870</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.910569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.509731</td>\n",
       "      <td>0.864372</td>\n",
       "      <td>0.869306</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.701700</td>\n",
       "      <td>0.864372</td>\n",
       "      <td>0.871615</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.892308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.907628</td>\n",
       "      <td>0.850202</td>\n",
       "      <td>0.859454</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.706767</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.806921</td>\n",
       "      <td>0.878543</td>\n",
       "      <td>0.885149</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.925620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.857961</td>\n",
       "      <td>0.878543</td>\n",
       "      <td>0.885494</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.841355</td>\n",
       "      <td>0.892713</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857712</td>\n",
       "      <td>0.888664</td>\n",
       "      <td>0.895389</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.860330</td>\n",
       "      <td>0.888664</td>\n",
       "      <td>0.895389</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1979/1979 [00:00<00:00, 2499.78 examples/s]\n",
      "Map: 100%|██████████| 497/497 [00:00<00:00, 2457.24 examples/s]\n",
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/362353729.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 32,846,217 || all params: 527,284,882 || trainable%: 6.2293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1230/1230 33:23, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Expert</th>\n",
       "      <th>F1 Novice</th>\n",
       "      <th>F1 Gemini</th>\n",
       "      <th>F1 Gpt4</th>\n",
       "      <th>F1 Llama31405b</th>\n",
       "      <th>F1 Llama318b</th>\n",
       "      <th>F1 Mistral</th>\n",
       "      <th>F1 Phi3</th>\n",
       "      <th>F1 Sonnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.262100</td>\n",
       "      <td>1.804829</td>\n",
       "      <td>0.325956</td>\n",
       "      <td>0.252573</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.364532</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.331839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.094200</td>\n",
       "      <td>0.698069</td>\n",
       "      <td>0.762575</td>\n",
       "      <td>0.768353</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.796460</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>0.818898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.587632</td>\n",
       "      <td>0.800805</td>\n",
       "      <td>0.811666</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.914729</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.697987</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.844828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.582063</td>\n",
       "      <td>0.841046</td>\n",
       "      <td>0.848981</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.624616</td>\n",
       "      <td>0.828974</td>\n",
       "      <td>0.833680</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.597938</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.814159</td>\n",
       "      <td>0.881890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.749373</td>\n",
       "      <td>0.855131</td>\n",
       "      <td>0.863377</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>0.809160</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.870229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.795278</td>\n",
       "      <td>0.853119</td>\n",
       "      <td>0.864516</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.760331</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.861789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.781912</td>\n",
       "      <td>0.869215</td>\n",
       "      <td>0.878279</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.892562</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.766917</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.873950</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.732779</td>\n",
       "      <td>0.867203</td>\n",
       "      <td>0.872618</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.776860</td>\n",
       "      <td>0.817391</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.879310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.716751</td>\n",
       "      <td>0.877264</td>\n",
       "      <td>0.884777</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.854701</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.879310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.713860</td>\n",
       "      <td>0.877264</td>\n",
       "      <td>0.884777</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.854701</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.879310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712852</td>\n",
       "      <td>0.877264</td>\n",
       "      <td>0.884811</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.854701</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.873950</td>\n",
       "      <td>0.879310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1981/1981 [00:00<00:00, 2528.71 examples/s]\n",
      "Map: 100%|██████████| 495/495 [00:00<00:00, 2678.14 examples/s]\n",
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/362353729.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 32,846,217 || all params: 527,284,882 || trainable%: 6.2293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 33:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Expert</th>\n",
       "      <th>F1 Novice</th>\n",
       "      <th>F1 Gemini</th>\n",
       "      <th>F1 Gpt4</th>\n",
       "      <th>F1 Llama31405b</th>\n",
       "      <th>F1 Llama318b</th>\n",
       "      <th>F1 Mistral</th>\n",
       "      <th>F1 Phi3</th>\n",
       "      <th>F1 Sonnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.308100</td>\n",
       "      <td>1.977020</td>\n",
       "      <td>0.195960</td>\n",
       "      <td>0.109279</td>\n",
       "      <td>0.388571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.323900</td>\n",
       "      <td>0.696683</td>\n",
       "      <td>0.783838</td>\n",
       "      <td>0.784123</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.813559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.592100</td>\n",
       "      <td>0.451712</td>\n",
       "      <td>0.842424</td>\n",
       "      <td>0.842811</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.740157</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.467679</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.879939</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.890756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.640544</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.867007</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.838235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.661983</td>\n",
       "      <td>0.896970</td>\n",
       "      <td>0.901870</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.957983</td>\n",
       "      <td>0.932203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.617238</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.904776</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.859813</td>\n",
       "      <td>0.787402</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.957983</td>\n",
       "      <td>0.915254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.584258</td>\n",
       "      <td>0.915152</td>\n",
       "      <td>0.922101</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.942149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.606306</td>\n",
       "      <td>0.913131</td>\n",
       "      <td>0.920688</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.924370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.602754</td>\n",
       "      <td>0.917172</td>\n",
       "      <td>0.924246</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.942149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.603797</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.926100</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.957983</td>\n",
       "      <td>0.942149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606165</td>\n",
       "      <td>0.915152</td>\n",
       "      <td>0.922344</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.957983</td>\n",
       "      <td>0.942149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1980/1980 [00:00<00:00, 2634.06 examples/s]\n",
      "Map: 100%|██████████| 496/496 [00:00<00:00, 2549.95 examples/s]\n",
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/362353729.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 32,846,217 || all params: 527,284,882 || trainable%: 6.2293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1230/1230 33:36, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Expert</th>\n",
       "      <th>F1 Novice</th>\n",
       "      <th>F1 Gemini</th>\n",
       "      <th>F1 Gpt4</th>\n",
       "      <th>F1 Llama31405b</th>\n",
       "      <th>F1 Llama318b</th>\n",
       "      <th>F1 Mistral</th>\n",
       "      <th>F1 Phi3</th>\n",
       "      <th>F1 Sonnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.100500</td>\n",
       "      <td>1.347003</td>\n",
       "      <td>0.508065</td>\n",
       "      <td>0.502130</td>\n",
       "      <td>0.707483</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.298137</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.901200</td>\n",
       "      <td>0.827824</td>\n",
       "      <td>0.731855</td>\n",
       "      <td>0.721331</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.818898</td>\n",
       "      <td>0.792079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.549500</td>\n",
       "      <td>0.609078</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.819175</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.897638</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.730435</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.842975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.524433</td>\n",
       "      <td>0.836694</td>\n",
       "      <td>0.846858</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.852174</td>\n",
       "      <td>0.918919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.750772</td>\n",
       "      <td>0.834677</td>\n",
       "      <td>0.845351</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.678261</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.919355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.664916</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.870913</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.892562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.781856</td>\n",
       "      <td>0.856855</td>\n",
       "      <td>0.860284</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.873950</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.854962</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.746349</td>\n",
       "      <td>0.868952</td>\n",
       "      <td>0.875221</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.949153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.720428</td>\n",
       "      <td>0.872984</td>\n",
       "      <td>0.878578</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.859813</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.706372</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.874444</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.859813</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.708351</td>\n",
       "      <td>0.872984</td>\n",
       "      <td>0.876243</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.859813</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.718893</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.874429</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.859813</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "steps = 5 if DEBUG else 50\n",
    "# train = train.sample(n=400)# if DEBUG else train\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f'Fold {fold+1}/5')\n",
    "\n",
    "    valid_df = train[train[\"fold\"] == fold]\n",
    "    train_df = train[train[\"fold\"] != fold]\n",
    "    \n",
    "    # from pandas\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    valid_ds = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    train_tokenized_ds = train_ds.map(preprocess_function, batched=True)\n",
    "    valid_tokenized_ds = valid_ds.map(preprocess_function, batched=True)\n",
    "    \n",
    "    base_model = Qwen2ForSequenceClassificationPlus.from_pretrained(\n",
    "        TARGET_MODEL,\n",
    "        num_labels=len(id2label),\n",
    "        id2label=id2label,          # Add this line\n",
    "        label2id=label2id,          # Add this line\n",
    "        device_map={\"\":0},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # Model\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        target_modules=['q_proj','k_proj','v_proj','up_proj','down_proj','gate_proj'],\n",
    "        # modules_to_save=[\"score\"],\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"///mnt/c/Personal/Competitions/BEA_2025/Qwen25_0.5/outputs/fold{fold}\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_grad_norm= 0.5,#0.3,\n",
    "        optim='paged_adamw_32bit',\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.0001,\n",
    "        save_total_limit=1,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps = 100,\n",
    "        eval_steps = 100,\n",
    "        logging_steps= 100,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        warmup_steps=10,\n",
    "        report_to='none', # if DEBUG else 'wandb',\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized_ds,\n",
    "        eval_dataset=valid_tokenized_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    # validation \n",
    "    pred_output = trainer.predict(valid_tokenized_ds)\n",
    "    logits = pred_output.predictions\n",
    "    probas = softmax(logits)\n",
    "    np.save(f'///mnt/c/Personal/Competitions/BEA_2025/Qwen25_0.5/outputs/fold{fold}.npy', probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer, model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18be2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda cache clear\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Processing Functions\n",
    "def load_test_data(test_data_path='///mnt/c/Personal/Competitions/BEA_2025/data/mrbench_v3_testset.json'):\n",
    "    \"\"\"\n",
    "    Load development and (optionally) test datasets\n",
    "    \"\"\"    \n",
    "    # Load test data if provided\n",
    "    test_examples = []\n",
    "    test_data = None\n",
    "    if test_data_path:\n",
    "        with open(test_data_path, 'r',encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "        \n",
    "        for dialogue in test_data:\n",
    "            conversation_id = dialogue[\"conversation_id\"]\n",
    "            conversation_history = dialogue[\"conversation_history\"]\n",
    "            \n",
    "            for tutor_id, tutor_data in dialogue[\"tutor_responses\"].items():\n",
    "                test_examples.append({\n",
    "                    \"conversation_id\": conversation_id,\n",
    "                    \"conversation_history\": conversation_history,\n",
    "                    \"tutor_response\": tutor_data[\"response\"],\n",
    "                    \"tutor_id\": tutor_id\n",
    "                })\n",
    "    \n",
    "    return test_examples, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>Question</th>\n",
       "      <th>Response</th>\n",
       "      <th>tutor_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>It looks like you've done a great job figuring...</td>\n",
       "      <td>Tutor_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>You've done a great job, but there's a small m...</td>\n",
       "      <td>Tutor_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>OK, read the question again, and answer these ...</td>\n",
       "      <td>Tutor_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Tutor: I see where you're coming from, but I t...</td>\n",
       "      <td>Tutor_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Great job! Can you explain how you arrived at ...</td>\n",
       "      <td>Tutor_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             conversation_id  \\\n",
       "0  1030-adb61831-0383-4e51-a673-ab978590f69b   \n",
       "1  1030-adb61831-0383-4e51-a673-ab978590f69b   \n",
       "2  1030-adb61831-0383-4e51-a673-ab978590f69b   \n",
       "3  1030-adb61831-0383-4e51-a673-ab978590f69b   \n",
       "4  1030-adb61831-0383-4e51-a673-ab978590f69b   \n",
       "\n",
       "                                            Question  \\\n",
       "0  Tutor: Hi, could you please provide a step-by-...   \n",
       "1  Tutor: Hi, could you please provide a step-by-...   \n",
       "2  Tutor: Hi, could you please provide a step-by-...   \n",
       "3  Tutor: Hi, could you please provide a step-by-...   \n",
       "4  Tutor: Hi, could you please provide a step-by-...   \n",
       "\n",
       "                                            Response tutor_id  \n",
       "0  It looks like you've done a great job figuring...  Tutor_1  \n",
       "1  You've done a great job, but there's a small m...  Tutor_2  \n",
       "2  OK, read the question again, and answer these ...  Tutor_3  \n",
       "3  Tutor: I see where you're coming from, but I t...  Tutor_4  \n",
       "4  Great job! Can you explain how you arrived at ...  Tutor_5  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples,test_data = load_test_data()\n",
    "test = pd.DataFrame(test_examples)\n",
    "test.rename(columns={'conversation_history':'Question','tutor_response':'Response'},inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64db7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in test.iterrows():\n",
    "    question = row.Question\n",
    "    response = row.Response\n",
    "    convid = row.conversation_id\n",
    "    context = test[(test.conversation_id==convid)&(test.Response!=response)].Response.values\n",
    "    context = ' [SEP] '.join(context)\n",
    "    test.loc[i, 'context'] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "436b8596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1547/1547 [00:00<00:00, 2387.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "test['input'] = \"Question: \" + test['Question'] + '; Answer: ' + test['Response'] + '; Context: ' + train[\"context\"]\n",
    "test_ds = Dataset.from_pandas(test)\n",
    "test_tokenized_ds = test_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5226e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [f'target_{i}' for i in range(len(id2label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7844a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Fold 0\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/1947117112.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Map: 100%|██████████| 494/494 [00:00<00:00, 2689.16 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Fold 1\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/1947117112.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Map: 100%|██████████| 494/494 [00:00<00:00, 3179.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Fold 2\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/1947117112.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Map: 100%|██████████| 497/497 [00:00<00:00, 2517.17 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Fold 3\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/1947117112.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Map: 100%|██████████| 495/495 [00:00<00:00, 2679.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Fold 4\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassificationPlus were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20770/1947117112.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Map: 100%|██████████| 496/496 [00:00<00:00, 2569.33 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CKPTS =  [\"checkpoint-700\", \"checkpoint-1000\", \"checkpoint-1200\", \"checkpoint-1100\", \"checkpoint-900\"]\n",
    "final_preds = []\n",
    "for fold, ckpt in enumerate(CKPTS):\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(f\"Fold {fold}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    \n",
    "    base_model = Qwen2ForSequenceClassificationPlus.from_pretrained(\n",
    "        TARGET_MODEL,\n",
    "        num_labels=len(id2label),\n",
    "        id2label=id2label,          # Add this line\n",
    "        label2id=label2id,          # Add this line\n",
    "        device_map={\"\":0},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, f\"///mnt/c/Personal/Competitions/BEA_2025/Qwen25_0.5/outputs/fold{fold}/{ckpt}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "    # valid\n",
    "    valid_df = train[train[\"fold\"] == fold]\n",
    "    idxs = valid_df.index\n",
    "    valid_ds = Dataset.from_pandas(valid_df)\n",
    "    valid_tokenized_ds = valid_ds.map(preprocess_function, batched=True)\n",
    "    \n",
    "    pred_output = trainer.predict(valid_tokenized_ds)\n",
    "    logits = pred_output.predictions\n",
    "    probas = softmax(logits)\n",
    "    train.loc[idxs, target_cols] = probas\n",
    "    \n",
    "    # test\n",
    "    \n",
    "    pred_output = trainer.predict(test_tokenized_ds)\n",
    "    logits = pred_output.predictions\n",
    "    probas = softmax(logits)\n",
    "    final_preds.append(probas)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[target_cols] = np.mean(final_preds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       3\n",
       "2       0\n",
       "3       4\n",
       "4       7\n",
       "       ..\n",
       "1542    8\n",
       "1543    7\n",
       "1544    3\n",
       "1545    2\n",
       "1546    6\n",
       "Name: pred, Length: 1547, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['pred'] = test[target_cols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[1])\n",
    "test['pred'] = test['pred'].astype(int)\n",
    "test['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 0, ..., 3, 2, 6], shape=(1547,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = test['pred'].values\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a39f5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('///mnt/c/Personal/Competitions/BEA_2025/Qwen25_0.5/outputs/test_probas.csv',index=\n",
    "            False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = []\n",
    "unique_conversation_ids = list(ex[\"conversation_id\"] for ex in test_examples)\n",
    "\n",
    "for conversation_id in unique_conversation_ids:\n",
    "    conversation_data = next(d for d in test_data if d[\"conversation_id\"] == conversation_id)\n",
    "    submission_item = {\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"conversation_history\": conversation_data[\"conversation_history\"],\n",
    "        \"tutor_responses\": {}\n",
    "    }\n",
    "        \n",
    "    for tutor_id, tutor_data in conversation_data[\"tutor_responses\"].items():\n",
    "        # Find the corresponding prediction\n",
    "        idx = next(i for i, ex in enumerate(test_examples) \n",
    "                    if ex[\"conversation_id\"] == conversation_id and ex[\"tutor_id\"] == tutor_id)\n",
    "        \n",
    "        predicted_class = id2label[pred_labels[idx]]\n",
    "        \n",
    "        submission_item[\"tutor_responses\"][tutor_id] = {\n",
    "            \"response\": tutor_data[\"response\"],\n",
    "            \"annotation\": {\n",
    "                \"Tutor_Identification\": predicted_class\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    submission.append(submission_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f7fb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"///mnt/c/Personal/Competitions/BEA_2025/Qwen25_0.5/outputs\", \"predictions.json\"), \"w\") as f:\n",
    "    json.dump(submission, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee1b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
